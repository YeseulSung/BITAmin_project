# -*- coding: utf-8 -*-
"""main_algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EayZICXBfhscVnJwVUG8s9j0RlAOjOsG
"""



#!pip install -q tensorflow-recommenders
#!pip install -q --upgrade tensorflow-datasets
import os
import sys
import gc
import glob
import joblib
from tqdm import tqdm
import pickle

import numpy as np
import pandas as pd

import keras
import tensorflow as tf
import tensorflow_recommenders as tfrs

from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Lambda, Input, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping, Callback
from tensorflow.keras.utils import plot_model

from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report
import tensorflow_datasets as tfds

import ast


def DCN(df, str_features, int_features, df_type = 'train'):
    str_features = ["user", "item_name", "item_brand", "item_category1", "item_category2", "item_category3"]
    int_features = ["review", "item_sale_price", "item_score", "item_count", "n1", "n2", "n3", "n4", "c6",
                    "total_score", "dur_score", "price_score", "design_score", "delivery_score", "options"]
    label_feature = ["like"]
    feature_names = str_features + int_features

    #feature type 변경 -> 여기 자꾸 오류떠서 밖에서 하고 오기 
    def setType(df):
        df['total_score'] = df['total_score'].apply(lambda x : x * 10)
        df['dur_score'] = df['dur_score'].apply(lambda x : x * 10)
        df['design_score'] = df['design_score'].apply(lambda x : x * 10)
        df['price_score'] = df['price_score'].apply(lambda x : x * 10)
        df['delivery_score'] = df['delivery_score'].apply(lambda x : x * 10)
        df['item_score'] = df['item_score'].apply(lambda x : x * 10)
        df['n1'] = df['n1'].apply(lambda x : x * 10) 
        df['n2'] = df['n2'].apply(lambda x : x * 10)
        df['n3'] = df['n3'].apply(lambda x : x * 10)
        df['n4'] = df['n4'].apply(lambda x : x * 10)

        df['total_score'] = np.asarray(df['total_score']).astype(np.int)
        df['dur_score'] = np.asarray(df['dur_score']).astype(np.int)
        df['design_score'] = np.asarray(df['design_score']).astype(np.int)
        df['delivery_score'] = np.asarray(df['delivery_score']).astype(np.int)
        df['price_score'] = np.asarray(df['price_score']).astype(np.int)
        df['item_score'] = np.asarray(df['item_score']).astype(np.int)
        df['n1'] = np.asarray(df['n1']).astype(np.int)
        df['n2'] = np.asarray(df['n2']).astype(np.int)
        df['n3'] = np.asarray(df['n3']).astype(np.int)
        df['n4'] = np.asarray(df['n4']).astype(np.int)

        for f in str_features:
          if df[f].dtypes == float:
            df[f] = np.asarray(df[f]).astype(np.int)

        for f in int_features:
          df[f] = np.asarray(df[f]).astype(np.int)
            
        return df
    
    # 데이터 dict로 변환
    def generateDict(df):
        # str features는 encoding
        train_str_dict = {
        str_feature: [str(val).encode() for val in df[str_feature].values]
        for str_feature in str_features
        }
        # int features는 int
        train_int_dict = {
            int_feature: df[int_feature].values
            for int_feature in int_features
            }

        # label columns이 있다면~
        try:
            train_label_dict = {
                'like' : df['like'].values
            }
            train_str_dict.update(train_label_dict)
        except:
            pass
        # try:
        #     train_label_dict = {
        #         'rating_per_user' : df['rating_per_user'].values
        #     }
        #     train_str_dict.update(train_label_dict)
        # except:
        #     pass
    
        train_str_dict.update(train_int_dict)
        
        return train_str_dict

    df_copy = setType(df)
    input_dict = generateDict(df_copy)

    # tensor
    tensor = tf.data.Dataset.from_tensor_slices(input_dict)
    cached = tensor.shuffle(100_000).batch(8192).cache()
    # unique data 저장
    
    # train data 일 때, 
    if df_type == 'train':
        vocabularies = {}
    
        for feature_name in tqdm(feature_names):
            vocab = tensor.batch(1_000_000).map(lambda x: x[feature_name])
            vocabularies[feature_name] = np.unique(np.concatenate(list(vocab)))
    
        return cached, vocabularies
      
    # test data 일 때, 
    else:
        return cached

class model(tfrs.Model):
    
    def __init__(self, cross_layer_sizes, deep_layer_sizes, learning_rate, vocabularies, emb_dim = 32, threshold = 0.5, projection_dim = None, metric = 'binary'):
        super().__init__()
    
        self.embedding_dimension = emb_dim
        str_features = ["user", "item_name", "item_brand", "item_category1", "item_category2", "item_category3"]
        int_features = ["review", "item_sale_price", "item_score", "item_count", "n1", "n2", "n3", "n4", "c6",
                        "total_score", "dur_score", "price_score", "design_score", "delivery_score", "options"]
        label_feature = ["like"]
        feature_names = str_features + int_features + label_feature
        self._all_features = str_features + int_features
        self._embeddings = {}
    
        # Compute embeddings for string features.
        for feature_name in str_features:
            vocabulary = vocabularies[feature_name]
            self._embeddings[feature_name] = tf.keras.Sequential(
                [tf.keras.layers.experimental.preprocessing.StringLookup(
                    vocabulary=vocabulary, mask_token=None),
                    tf.keras.layers.Embedding(len(vocabulary) + 1,
                    self.embedding_dimension)
                                             ])
          
    
        # Compute embeddings for int features.
        for feature_name in int_features:
            vocabulary = vocabularies[feature_name]
            self._embeddings[feature_name] = tf.keras.Sequential(
                [tf.keras.layers.experimental.preprocessing.IntegerLookup(
                    vocabulary=vocabulary, mask_value=None),
                    tf.keras.layers.Embedding(len(vocabulary) + 1,
                    self.embedding_dimension)
                                         ])
    
#         if use_cross_layer:
#             self._cross_layer = tfrs.layers.dcn.Cross(
#                 projection_dim = projection_dim,
#                 kernel_initializer = "glorot_uniform")
#         else:
#             self._cross_layer = None
        
        # Cross layer
        if cross_layer_sizes:
            self._cross_layer = [tfrs.layers.dcn.Cross(
                projection_dim = projection_dim,
                kernel_initializer = "glorot_uniform") for _ in range(cross_layer_sizes)]
        else:
            self._cross_layer = None
            
        # Deep layer
        self._deep_layers = [tf.keras.layers.Dense(layer_size, activation="relu")
            for layer_size in deep_layer_sizes]
        
        # Output layer
        self._logit_layer = tf.keras.layers.Dense(1,
                                                  activation = 'sigmoid'
                                                  )
        # Metric
        if metric == 'binary':
            self.task = tfrs.tasks.Ranking(
            loss = tf.keras.losses.BinaryCrossentropy(),
            metrics=[
                    tf.keras.metrics.BinaryAccuracy(
                        name='binary_accuracy', dtype = None, threshold = threshold)
                    ])
                    
        elif metric == 'reg':
            self.task = tfrs.tasks.Ranking(
                loss=tf.keras.losses.MeanSquaredError(),
                metrics=[
                    tf.keras.metrics.RootMeanSquaredError("RMSE")
                    ])
        else:
            print('metric ERROR!')
            sys.exit(1)
    
    def call(self, features):
        # Concatenate embeddings
        embeddings = []
        for feature_name in self._all_features:
            embedding_fn = self._embeddings[feature_name]
            embeddings.append(embedding_fn(features[feature_name]))
    
        x = tf.concat(embeddings, axis=1)
    
        # Build Cross Network
#         if self._cross_layer is not None:
#             x = self._cross_layer(x)
        for cross_layer in self._cross_layer:
            x = cross_layer(x)
        
        # Build Deep Network
        for deep_layer in self._deep_layers:
            x = deep_layer(x)
    
        return self._logit_layer(x)
    
    def compute_loss(self, features, training=False, metric = 'binary'):
        if metric == 'binary':
            labels = features.pop("like")
        elif metric == 'reg':
            labels = features.pop("rating_per_user")
        scores = self(features)
    
        return self.task(
            labels=labels,
            predictions=scores,
            )

def recommendation1(userID, category, model, item_list, for_user):
    str_features = ["user", "item_name", "item_brand", "item_category1", "item_category2", "item_category3"]
    int_features = ["review", "item_sale_price", "item_score", "item_count", "n1", "n2", "n3", "n4", "c6",
                    "total_score", "dur_score", "price_score", "design_score", "delivery_score", "options"]
    label_feature = ["like"]
    feature_names = str_features + int_features + label_feature
    #item : original df
    #item_list = pd.read_csv("/content/drive/Shareddrives/컨퍼런스/item_list.csv", encoding = "utf-8-sig")
    nlist = item_list.query("user == @userID")["item_name"].value_counts().index.tolist()
    #for_user = pd.read_csv("/content/drive/Shareddrives/컨퍼런스/for_user_final_0204.csv", encoding = "utf-8-sig")
    new = for_user.query("item_name not in @nlist").reset_index(drop = True)
    new['user'] = userID
    if len(nlist) != 0:
        #cached = preprocessing(item_copy, str_features, int_features, df_type = 'test')
        cached = DCN(new, str_features, int_features, df_type = 'test')
        pred = model.predict(cached)
        pred = pd.DataFrame(pred).sort_values(0, ascending = False).reset_index()
        pred.columns = ['item_name', 'prob']
        pred["c6"] = new.loc[pred["item_name"], "c6"].values
        pred["item_name"] = new.loc[pred["item_name"], "item_name"].values
        if category != 0:
            pred1 = pred.query("c6 == @category and prob >= 0.75")
            pred1["notice"] = "카테고리 추천 결과"
            if len(pred1) > 10:
                pred1 = pred1.iloc[:10, :]
            pred2 = pred.query("c6 != @category and prob >= 0.75")
            pred2["notice"] = "그외 눈여겨 볼 만한 상품"
            if len(pred2) > 10:
                pred2 = pred2.iloc[:10, :]
            pred = pd.concat([pred1, pred2], axis= 0).reset_index(drop = True)
        else:
            pred = pred.query("prob >= 0.75").reset_index(drop = True)
            pred["notice"] = "카테고리 추천 결과"
            if len(pred) > 20:
                pred = pred.iloc[:20, :]
        pred = pred.loc[:, ["notice", "item_name", "prob", "c6"]]

    else:
        new = new.sort_values(by = ["item_score", "item_count"], ascending = [False, False]).reset_index(drop = True)
        if category != 0:
            pred1 = new.query("c6 == @category").reset_index(drop = True).iloc[:10, :]
            pred1["notice"] = "카테고리 추천 결과"
            pred2 = new.query("c6 != @category").reset_index(drop = True).iloc[:10, :]
            pred2["notice"] = "그외 눈여겨 볼 만한 상품"
            pred = pd.concat([pred1, pred2], axis= 0).reset_index(drop = True)
        else:
            pred = new.iloc[:20, :]
            pred["notice"] = "카테고리 추천 결과"
        pred = pred.loc[:, ["notice", "item_name", "item_score", "c6"]]
        

    return pred

